{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e8e68e",
   "metadata": {},
   "source": [
    "가장 대표적인 sequence 데이터가 문자열이라는 이야기를 저번에 했었죠?!\n",
    "문자열을 어떻게 전처리하면 되는지 순서를 적어보겠습니다.\n",
    "\n",
    "1. strings을 token으로 분리해준다.\n",
    "2. table of vocabulary를 만들어서, 나누어준 token들을 숫자와 매핑해준다.\n",
    "3. text를 숫자로 구성된 sequence로 바꿔서 더 다루기 쉽게 만들어 주는 것이죠!\n",
    "\n",
    "자, 그럼 해볼까요?!\n",
    "\n",
    "# 8.2.1. Reading the Dataset\n",
    "\n",
    "일단 불러와야 겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf80504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# text lines: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# text lines: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df5ae1",
   "metadata": {},
   "source": [
    "# 8.2.2. Tokenization\n",
    "text sequence로 구성된 list(lines)을 입력으로 받는 함수입니다~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba842662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"\n",
    "    Token lines들을 word, character 단위의 토큰으로 분리해준다.\n",
    "    \"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: '+token)\n",
    "        \n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad866118",
   "metadata": {},
   "source": [
    "# 8.2.3. Vocabulary\n",
    "\n",
    "corpus, 즉 어원을 찾아서 단어들이 담긴 bag을 만들 것 입니다.\n",
    "어원이 없다면 unk 토큰으로 받을게요!\n",
    "bos; beginning of sentence\n",
    "eos; end of a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c94906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Vocabulary for text.\n",
    "    \"\"\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
