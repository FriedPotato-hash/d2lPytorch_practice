{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed81ae78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0426, -0.2457, -0.0303,  0.1832,  0.0788, -0.0737,  0.3692, -0.1708,\n",
       "         -0.1082, -0.0370],\n",
       "        [-0.0799, -0.2310, -0.0334,  0.1268,  0.0025, -0.0847,  0.1970, -0.0632,\n",
       "         -0.0298,  0.0762]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20,256), nn.ReLU(), nn.Linear(256,10))\n",
    "\n",
    "X = torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214165c",
   "metadata": {},
   "source": [
    "### 5.1.1 A Custom Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898d9efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0715,  0.1927,  0.1439, -0.0932, -0.0532, -0.1742, -0.1029, -0.1537,\n",
       "          0.0158, -0.1765],\n",
       "        [ 0.1853,  0.3218,  0.1675, -0.0182,  0.0048, -0.0685,  0.0464, -0.1833,\n",
       "          0.1286, -0.0530]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    # connect layers\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Call the Call the constructor of the 'MLP' parent class 'Module' to perfrom\n",
    "        the necessary initialization.In this way, other function arguments \n",
    "        can also be specified during class instantiation, such as the model parameters, 'params'\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "    \n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa7aad",
   "metadata": {},
   "source": [
    "### 5.1.2 The Sequential Block\n",
    "\n",
    "In order to make Sequential, we need two functions.\n",
    "1. a function to append blocks one by one to a list.\n",
    "2. A forward propagation function to pass an input through the chain of blocks, in the same order as they were appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932a79c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1092, -0.1527,  0.1487, -0.1246, -0.0436,  0.0195, -0.1831, -0.0044,\n",
       "         -0.0608,  0.0662],\n",
       "        [-0.0462, -0.2063,  0.2539, -0.0981,  0.0628,  0.1115, -0.2262, -0.0914,\n",
       "          0.0724,  0.2137]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            '''\n",
    "            'module'is an instance  of a 'Module' subclass.\n",
    "            We save it in the member of variable '_modules' of the Module class\n",
    "            its type is Ordered Dictionary\n",
    "            '''\n",
    "            self._modules[str(idx)]=module\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "    \n",
    "net = MySequential(nn.Linear(20,256), nn.ReLU(), nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60d4a7",
   "metadata": {},
   "source": [
    "### 5.1.3 Executing Code in the Forward Propagation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815d9693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1435, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not comput gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20,20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight)+1)\n",
    "        \n",
    "        X = self.linear(X)\n",
    "        \n",
    "        while X.abs().sum()>1:\n",
    "            X/=2\n",
    "        return X.sum()\n",
    "    \n",
    "# while 돌리는 구간은 사실상, 현실의 어떤 task에도 사용되지 않음.\n",
    "# 즉, 중요한거는 이 흐름을 파악하는것.\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76786e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1180, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20,64), nn.ReLU(),\n",
    "                                nn.Linear(64,32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32,16)\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "    \n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16,20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61db76d",
   "metadata": {},
   "source": [
    "### Built in initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cabe4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all the parameters to a Gaussian random variables with standard deviation 0.01, bias = 0\n",
    "def init_normal(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "\n",
    "# Initialize all the parameters to a given constant value\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "\n",
    "# Also can apply different initializers for certain blocks.\n",
    "def xavier(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "def init_42(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b42fc",
   "metadata": {},
   "source": [
    "### Custom Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6192bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_init(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        print(\n",
    "        \"Init\",\n",
    "        *[(name, param.shape) for name, param in m.named_parameters()[0]])\n",
    "    \n",
    "        nn.init.unifor_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7daaf6",
   "metadata": {},
   "source": [
    "### Tied Parameters\n",
    "shared layer의 weight는 모두 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82d874b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1813],\n",
       "        [0.1813],\n",
       "        [0.1730],\n",
       "        [0.1812],\n",
       "        [0.1859],\n",
       "        [0.1815],\n",
       "        [0.1818],\n",
       "        [0.1756],\n",
       "        [0.1811],\n",
       "        [0.1833],\n",
       "        [0.1816],\n",
       "        [0.1855],\n",
       "        [0.1814],\n",
       "        [0.1805],\n",
       "        [0.1820],\n",
       "        [0.1816],\n",
       "        [0.1807],\n",
       "        [0.1814],\n",
       "        [0.1813],\n",
       "        [0.1802]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(20,4)\n",
    "\n",
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8,1))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fa650",
   "metadata": {},
   "source": [
    "# 5.3 Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3d112",
   "metadata": {},
   "source": [
    "### 5.3.1 Layers without Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7089c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X-X.mean()\n",
    "    \n",
    "net = nn.Sequential(nn.Linear(8,128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be82ec",
   "metadata": {},
   "source": [
    "### 5.3.2 Layers with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6494b8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.6415],\n",
       "        [4.7749]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self,X):\n",
    "        linear = torch.matmul(X, self.weight.data)+self.bias.data\n",
    "        return F.relu(linear)\n",
    "linear = MyLinear(5,3)\n",
    "linear.weight\n",
    "\n",
    "net = nn.Sequential(MyLinear(64,8), MyLinear(8,1))\n",
    "net(torch.rand(2,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08a389",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2722ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [1.3620]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Design a layer that takes an input and computes a tensor reduction\n",
    "class ex1_Layer(nn.Module):\n",
    "    def __init__(self, in_units, units, ):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "    def forward(self,X):\n",
    "        linear = torch.matmul(X, self.weight.data)\n",
    "        return F.relu(linear)\n",
    "net = nn.Sequential(ex1_Layer(20,4), ex1_Layer(4,1))\n",
    "net(torch.rand(2,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b182d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Design a layer that returns the leading half of the Fourier coefficients of the data.\n",
    "\n",
    "import numpy as np\n",
    "def fourier_series_coeff_numpy(f, T, N, return_complex=False):\n",
    "    \"\"\"Calculates the first 2*N+1 Fourier series coeff. of a periodic function.\n",
    "\n",
    "    Given a periodic, function f(t) with period T, this function returns the\n",
    "    coefficients a0, {a1,a2,...},{b1,b2,...} such that:\n",
    "\n",
    "    f(t) ~= a0/2+ sum_{k=1}^{N} ( a_k*cos(2*pi*k*t/T) + b_k*sin(2*pi*k*t/T) )\n",
    "\n",
    "    If return_complex is set to True, it returns instead the coefficients\n",
    "    {c0,c1,c2,...}\n",
    "    such that:\n",
    "\n",
    "    f(t) ~= sum_{k=-N}^{N} c_k * exp(i*2*pi*k*t/T)\n",
    "\n",
    "    where we define c_{-n} = complex_conjugate(c_{n})\n",
    "\n",
    "    Refer to wikipedia for the relation between the real-valued and complex\n",
    "    valued coeffs at http://en.wikipedia.org/wiki/Fourier_series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : the periodic function, a callable like f(t)\n",
    "    T : the period of the function f, so that f(0)==f(T)\n",
    "    N_max : the function will return the first N_max + 1 Fourier coeff.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if return_complex == False, the function returns:\n",
    "\n",
    "    a0 : float\n",
    "    a,b : numpy float arrays describing respectively the cosine and sine coeff.\n",
    "\n",
    "    if return_complex == True, the function returns:\n",
    "\n",
    "    c : numpy 1-dimensional complex-valued array of size N+1\n",
    "\n",
    "    \"\"\"\n",
    "    # From Shanon theoreom we must use a sampling freq. larger than the maximum\n",
    "    # frequency you want to catch in the signal.\n",
    "    f_sample = 2 * N\n",
    "    # we also need to use an integer sampling frequency, or the\n",
    "    # points will not be equispaced between 0 and 1. We then add +2 to f_sample\n",
    "    t, dt = np.linspace(0, T, f_sample + 2, endpoint=False, retstep=True)\n",
    "\n",
    "    y = np.fft.rfft(f(t)) / t.size\n",
    "\n",
    "    if return_complex:\n",
    "        return y\n",
    "    else:\n",
    "        y *= 2\n",
    "        return y[0].real, y[1:-1].real, -y[1:-1].imag\n",
    "    \n",
    "class ex2_Layer(nn.Module):\n",
    "    def __init__(self, in_units, units, ):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "    def forward(self,X):\n",
    "        linear = fourier_series_coeff_numpy(f, T, N)\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc16892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
