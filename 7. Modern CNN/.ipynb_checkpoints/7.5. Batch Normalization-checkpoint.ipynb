{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5f98bc",
   "metadata": {},
   "source": [
    "Training Deep Neural Network is difficult and getting them to converge in a reasonable amount of time can be tricky.\n",
    "\n",
    "Batch Normalization accelerates the convergence of deep networks. \n",
    "This made it possible for practitioners to routinely train networks with over 100 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed43af",
   "metadata": {},
   "source": [
    "# 7.5.1. Training Deep Networks\n",
    "\n",
    "### Practical Challenges arised when training ML and NN\n",
    "1. data Preprocessing 과정에 따라 결과에 큰 차이 존재.\n",
    "2. variables in intermediate layer가 큰 범주 안에 존재할 수 있음\n",
    "3. 깊은 네트워크는 복잡하고 쉽게 overfitting 됨. Regularization이 더 중요해짐.\n",
    "\n",
    "BN은 개별 layer에 적용된다. 각 training iteration에서 처음 우리는 normalization of inputs을 해주는데, 이는 mean을 빼주고, standard deviation으로 나눠주는 것.\n",
    "\n",
    "그 다음 scale coefficient와 scale offset을 적용한다.\n",
    "\n",
    "만약, size가 1인 minibatch로 batch normalization을 진행한다면, 아무것도 얻을 수 있는 것이 없다. 이는, mean을 빼줄 때 0이 되기 때문 (그 자신이 mean이 되니까) 따라서, 충분한 크기의 mini batch를 사용해주어야 적절하다. \n",
    "\n",
    "* various source로부터 생성되는 noise는 최적화를 더 '빠르고', '과잉 최적화'를 줄이게 돕는다.\n",
    "\n",
    "어쩌면, variation이 regularization의 역할을 하는 것일수도 있음.\n",
    "minibatch는 따라서 50~100정도의 size일때가 가장 batch normalization이 잘 먹히는 구간이다.\n",
    "\n",
    "* 왜 우리는 sample들의 mean, standard deviation이 아니라, 전체의 (미니배치 기준으로의) mean과 standard deviation을 사용하는가?\n",
    "\n",
    "Once training is complete, why would we want the same image to be classified differently, depending on the batch in which it happens to reside? During training, such exact calculation is infeasible because the intermediate variables for all data examples change every time we update our model. However, once the model is trained, we can calculate the means and variances of each layer’s variables based on the entire dataset. Indeed this is standard practice for models employing batch normalization and thus batch normalization layers function differently\n",
    "\n",
    "* training mode *(normalizing by minibatch statistics) \n",
    "\n",
    "* prediction mode *(normalizing by dataset statistics)\n",
    "\n",
    "## 7.5.2.1. Fully Connected Layers\n",
    "the original paper inserts batch normalization after the affine transformation and before the nonlinear activation function (later applications may insert batch normalization right after activation functions)\n",
    "\n",
    "output = activation_fun(batch_norm(Wx+b))\n",
    "\n",
    "## 7.5.2.2. Convolution Layers\n",
    "Conv Layer에서도 마찬가지로 BN을 nonlinear activation function 이전에 적용한다. => 각 채널별로 BN은 따로해줘야 함.\n",
    "\n",
    "When the convolution has multiple output channels, we need to carry out batch normalization for each of the outputs of these channels, and each channel has its own scale and shift parameters, both of which are scalars. Assume that our minibatches contain  𝑚  examples and that for each channel, the output of the convolution has height  𝑝  and width  𝑞 . For convolutional layers, we carry out each batch normalization over the  𝑚⋅𝑝⋅𝑞  elements per output channel simultaneously. Thus, we collect the values over all spatial locations when computing the mean and variance and consequently apply the same mean and variance within a given channel to normalize the value at each spatial location.\n",
    "\n",
    "## 7.5.2.3. Batch Normalization During Prediction\n",
    "batch normalization은 training mode와 prediction mode에서 다르게 적용된다. \n",
    "1. noise in sample mean and sample variance arising from estimating each on minibatches는 더이상, 우리가 훈련시킨 모델에 desirable하지 않게 된다. \n",
    "2. BN을 할 수 있을 luxury of computing  없을수도 >,< -> YOLO같은.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877881bd",
   "metadata": {},
   "source": [
    "# 7.5.3. Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2caf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from d2l import nn as d2l\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # \"is_grad_enabled\"를 통해, 지금이 training인지, prediction 중인지 알 수 있다.\n",
    "    if not torch.is_grad_enabled():\n",
    "        # Prediction mode일 때,\n",
    "        X_hat = (X - moving_variance) / torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        # Training mode일 때,\n",
    "        assert len(X.shape) in (2,4)\n",
    "        \n",
    "        if len(X.shape) == 2: \n",
    "            # Fully-Connected layer의 경우,\n",
    "            # feature dimension으로 mean과 var 계산하라.\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X-mean)**2).mean(dim=0)\n",
    "        else: \n",
    "            # two-dimensional Conv layer의 경우,\n",
    "            # channel dimension(axis=1)으로 계산하라.\n",
    "            mean = X.mean(dim=(0,2,3), keepdim=True)\n",
    "            var = ((X-mean)**2).mean(dime=(0,2,3), keepdim=True)\n",
    "        X_hat = (X-mean)/torch.sqrt(var+eps)\n",
    "        \n",
    "        moving_mean = momentum * moving_mean + (1.0-momentum)*mean\n",
    "        moving_var = momentum * moving_mean + (1.0-momentum)*var\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
