{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5f98bc",
   "metadata": {},
   "source": [
    "Training Deep Neural Network is difficult and getting them to converge in a reasonable amount of time can be tricky.\n",
    "\n",
    "Batch Normalization accelerates the convergence of deep networks. \n",
    "This made it possible for practitioners to routinely train networks with over 100 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed43af",
   "metadata": {},
   "source": [
    "# 7.5.1. Training Deep Networks\n",
    "\n",
    "### Practical Challenges arised when training ML and NN\n",
    "1. data Preprocessing ê³¼ì •ì— ë”°ë¼ ê²°ê³¼ì— í° ì°¨ì´ ì¡´ì¬.\n",
    "2. variables in intermediate layerê°€ í° ë²”ì£¼ ì•ˆì— ì¡´ì¬í•  ìˆ˜ ìˆìŒ\n",
    "3. ê¹Šì€ ë„¤íŠ¸ì›Œí¬ëŠ” ë³µì¡í•˜ê³  ì‰½ê²Œ overfitting ë¨. Regularizationì´ ë” ì¤‘ìš”í•´ì§.\n",
    "\n",
    "BNì€ ê°œë³„ layerì— ì ìš©ëœë‹¤. ê° training iterationì—ì„œ ì²˜ìŒ ìš°ë¦¬ëŠ” normalization of inputsì„ í•´ì£¼ëŠ”ë°, ì´ëŠ” meanì„ ë¹¼ì£¼ê³ , standard deviationìœ¼ë¡œ ë‚˜ëˆ ì£¼ëŠ” ê²ƒ.\n",
    "\n",
    "ê·¸ ë‹¤ìŒ scale coefficientì™€ scale offsetì„ ì ìš©í•œë‹¤.\n",
    "\n",
    "ë§Œì•½, sizeê°€ 1ì¸ minibatchë¡œ batch normalizationì„ ì§„í–‰í•œë‹¤ë©´, ì•„ë¬´ê²ƒë„ ì–»ì„ ìˆ˜ ìˆëŠ” ê²ƒì´ ì—†ë‹¤. ì´ëŠ”, meanì„ ë¹¼ì¤„ ë•Œ 0ì´ ë˜ê¸° ë•Œë¬¸ (ê·¸ ìì‹ ì´ meanì´ ë˜ë‹ˆê¹Œ) ë”°ë¼ì„œ, ì¶©ë¶„í•œ í¬ê¸°ì˜ mini batchë¥¼ ì‚¬ìš©í•´ì£¼ì–´ì•¼ ì ì ˆí•˜ë‹¤. \n",
    "\n",
    "* various sourceë¡œë¶€í„° ìƒì„±ë˜ëŠ” noiseëŠ” ìµœì í™”ë¥¼ ë” 'ë¹ ë¥´ê³ ', 'ê³¼ì‰ ìµœì í™”'ë¥¼ ì¤„ì´ê²Œ ë•ëŠ”ë‹¤.\n",
    "\n",
    "ì–´ì©Œë©´, variationì´ regularizationì˜ ì—­í• ì„ í•˜ëŠ” ê²ƒì¼ìˆ˜ë„ ìˆìŒ.\n",
    "minibatchëŠ” ë”°ë¼ì„œ 50~100ì •ë„ì˜ sizeì¼ë•Œê°€ ê°€ì¥ batch normalizationì´ ì˜ ë¨¹íˆëŠ” êµ¬ê°„ì´ë‹¤.\n",
    "\n",
    "* ì™œ ìš°ë¦¬ëŠ” sampleë“¤ì˜ mean, standard deviationì´ ì•„ë‹ˆë¼, ì „ì²´ì˜ (ë¯¸ë‹ˆë°°ì¹˜ ê¸°ì¤€ìœ¼ë¡œì˜) meanê³¼ standard deviationì„ ì‚¬ìš©í•˜ëŠ”ê°€?\n",
    "\n",
    "Once training is complete, why would we want the same image to be classified differently, depending on the batch in which it happens to reside? During training, such exact calculation is infeasible because the intermediate variables for all data examples change every time we update our model. However, once the model is trained, we can calculate the means and variances of each layerâ€™s variables based on the entire dataset. Indeed this is standard practice for models employing batch normalization and thus batch normalization layers function differently\n",
    "\n",
    "* training mode *(normalizing by minibatch statistics) \n",
    "\n",
    "* prediction mode *(normalizing by dataset statistics)\n",
    "\n",
    "## 7.5.2.1. Fully Connected Layers\n",
    "the original paper inserts batch normalization after the affine transformation and before the nonlinear activation function (later applications may insert batch normalization right after activation functions)\n",
    "\n",
    "output = activation_fun(batch_norm(Wx+b))\n",
    "\n",
    "## 7.5.2.2. Convolution Layers\n",
    "Conv Layerì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ BNì„ nonlinear activation function ì´ì „ì— ì ìš©í•œë‹¤. => ê° ì±„ë„ë³„ë¡œ BNì€ ë”°ë¡œí•´ì¤˜ì•¼ í•¨.\n",
    "\n",
    "When the convolution has multiple output channels, we need to carry out batch normalization for each of the outputs of these channels, and each channel has its own scale and shift parameters, both of which are scalars. Assume that our minibatches contain  ğ‘š  examples and that for each channel, the output of the convolution has height  ğ‘  and width  ğ‘ . For convolutional layers, we carry out each batch normalization over the  ğ‘šâ‹…ğ‘â‹…ğ‘  elements per output channel simultaneously. Thus, we collect the values over all spatial locations when computing the mean and variance and consequently apply the same mean and variance within a given channel to normalize the value at each spatial location.\n",
    "\n",
    "## 7.5.2.3. Batch Normalization During Prediction\n",
    "batch normalizationì€ training modeì™€ prediction modeì—ì„œ ë‹¤ë¥´ê²Œ ì ìš©ëœë‹¤. \n",
    "1. noise in sample mean and sample variance arising from estimating each on minibatchesëŠ” ë”ì´ìƒ, ìš°ë¦¬ê°€ í›ˆë ¨ì‹œí‚¨ ëª¨ë¸ì— desirableí•˜ì§€ ì•Šê²Œ ëœë‹¤. \n",
    "2. BNì„ í•  ìˆ˜ ìˆì„ luxury of computing  ì—†ì„ìˆ˜ë„ >,< -> YOLOê°™ì€.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877881bd",
   "metadata": {},
   "source": [
    "# 7.5.3. Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2caf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from d2l import nn as d2l\n",
    "\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # \"is_grad_enabled\"ë¥¼ í†µí•´, ì§€ê¸ˆì´ trainingì¸ì§€, prediction ì¤‘ì¸ì§€ ì•Œ ìˆ˜ ìˆë‹¤.\n",
    "    if not torch.is_grad_enabled():\n",
    "        # Prediction modeì¼ ë•Œ,\n",
    "        X_hat = (X - moving_variance) / torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        # Training modeì¼ ë•Œ,\n",
    "        assert len(X.shape) in (2,4)\n",
    "        \n",
    "        if len(X.shape) == 2: \n",
    "            # Fully-Connected layerì˜ ê²½ìš°,\n",
    "            # feature dimensionìœ¼ë¡œ meanê³¼ var ê³„ì‚°í•˜ë¼.\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X-mean)**2).mean(dim=0)\n",
    "        else: \n",
    "            # two-dimensional Conv layerì˜ ê²½ìš°,\n",
    "            # channel dimension(axis=1)ìœ¼ë¡œ ê³„ì‚°í•˜ë¼.\n",
    "            mean = X.mean(dim=(0,2,3), keepdim=True)\n",
    "            var = ((X-mean)**2).mean(dime=(0,2,3), keepdim=True)\n",
    "        X_hat = (X-mean)/torch.sqrt(var+eps)\n",
    "        \n",
    "        moving_mean = momentum * moving_mean + (1.0-momentum)*mean\n",
    "        moving_var = momentum * moving_mean + (1.0-momentum)*var\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
